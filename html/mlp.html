<HTML>
<BODY>
<PRE>
<!-- Manpage converted by man2html 3.0.1 -->


</PRE>
<H4>NAME</H4><PRE>
       mlp - train a multilayer perceptron with backprop


</PRE>
<H4>SYNOPSIS</H4><PRE>
       <B>mlp</B> <B>-help</B>
         or
       <B>mlp</B>    <B>[-dfile</B>  <I>string</I><B>]</B>  <B>[-steps</B>  <I>integer</I><B>]</B> <B>[-seed</B> <I>integer</I><B>]</B>
              <B>[-freq</B> <I>integer</I><B>]</B> <B>[-numin</B> <I>integer</I><B>]</B> <B>[-numhid</B>  <I>integer</I><B>]</B>
              <B>[-numout</B>  <I>integer</I><B>]</B>  <B>[-lrate</B> <I>double</I><B>]</B> <B>[-mrate</B> <I>double</I><B>]</B>
              <B>[-winit</B> <I>double</I><B>]</B> <B>[-linout]</B> <B>[-pdump]</B> <B>[-gdump]</B>


</PRE>
<H4>DESCRIPTION</H4><PRE>
       Train a multilayer perceptron with a single  hidden  layer
       of  neurons on a set of data contained in a file using the
       backpropagation learning algorithm with momentum.   Output
       units  can  be  linear or sigmoidal, allowing you to model
       both discrete and continuous output target values.


</PRE>
<H4>OPTIONS</H4><PRE>
       <B>-dfile</B> <I>string</I>
              Training data file.

       <B>-steps</B> <I>integer</I>
              Number of simulated steps.

       <B>-seed</B> <I>integer</I>
              Random seed for initial state.

       <B>-freq</B> <I>integer</I>
              Status print frequency.

       <B>-numin</B> <I>integer</I>
              Number of inputs.

       <B>-numhid</B> <I>integer</I>
              Number of hidden nodes.

       <B>-numout</B> <I>integer</I>
              Number of outputs.

       <B>-lrate</B> <I>double</I>
              Learning rate.

       <B>-mrate</B> <I>double</I>
              Momentum rate.

       <B>-winit</B> <I>double</I>
              Weight init factor

       <B>-linout</B>
              Use linear outputs?

       <B>-pdump</B> Dump patterns at end of run?


       <B>-gdump</B> Dump gnuplot commands at end?


</PRE>
<H4>MISCELLANY</H4><PRE>
       The number of inputs and outputs must agree with the  for-
       mat  of  your  training data file.  The program expects to
       find training patterns listed one after another with  each
       training  pattern consisting of the inputs followed by the
       target outputs.

       If the -pdump switch  is  used,  then  the  patterns  will
       printed  to  stdout.  Hence,redirect this to a file if you
       want to save it.

       You should always use linear outputs if your target values
       are continuous.

       The  error  value  displayed  via  stderr is the root mean
       squared error taken over the entire data step.   Calculat-
       ing  this  error  measure  is typically far more expensive
       than a single training step, so you may wish  to  use  the
       -freq option to make it happen less frequently.

       If  you  network  doesn't converge to anything useful, try
       increasing the number of hidden nodes.  Moreover, you  may
       need  to  tweak the learning rate and momentum term.  This
       is just one of the curses of backprop.


</PRE>
<H4>BUGS</H4><PRE>
       The -gplot switch isn't very useful as it  only  works  on
       the first output neuron.

       No  sanity  checks  are performed to make sure that any of
       the options make sense.


</PRE>
<H4>AUTHOR</H4><PRE>
       Copyright (c) 1997, Gary William Flake.

       Permission granted for any use according to  the  standard
       GNU ``copyleft'' agreement provided that the author's com-
       ments are neither modified nor removed.   No  warranty  is
       given or implied.

</PRE>
<HR>
<ADDRESS>
Man(1) output converted with
<a href="http://www.oac.uci.edu/indiv/ehood/man2html.html">man2html</a>
</ADDRESS>
</BODY>
</HTML>
